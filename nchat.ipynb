{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a21033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sheets: ['userAuserB']\n",
      "Shape: (22, 4)\n",
      "Columns: ['Conversation ID', 'Timestamp', 'Sender', 'Message']\n",
      "   Conversation ID           Timestamp  Sender  \\\n",
      "0                1 2025-10-07 10:15:12  User B   \n",
      "1                1 2025-10-07 10:15:45  User A   \n",
      "2                1 2025-10-07 10:16:05  User B   \n",
      "3                1 2025-10-07 10:16:38  User A   \n",
      "4                1 2025-10-07 10:17:01  User B   \n",
      "\n",
      "                                             Message  \n",
      "0  \"Hey, did you see the client's feedback on the...  \n",
      "1  \"Just saw it. They want a lot of changes to th...  \n",
      "2  \"Yeah, that's what I was thinking. It's a big ...  \n",
      "3  \"I'll start on the revisions. Can you update t...  \n",
      "4  \"Will do. I'll block out the rest of the week ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "EXCEL_PATH = \"/home/pushpit-saluja/ML/salujapushpit/conversationfile.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "xls = pd.ExcelFile(EXCEL_PATH)\n",
    "print(\"Available sheets:\", xls.sheet_names)\n",
    "\n",
    "# Load first sheet\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=0)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05dfde78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected senders: ['user b' 'user a']\n",
      "→ Interpreted User A as: user a, User B as: user b\n",
      "✅ Total pairs created: 10\n",
      "                                          input_text  \\\n",
      "0  \"Hey, did you see the client's feedback on the...   \n",
      "1  \"Yeah, that's what I was thinking. It's a big ...   \n",
      "2                          \"Any plans for Saturday?\"   \n",
      "3   \"Oh, the one near the park? I heard it's great.\"   \n",
      "4                          \"Sounds good! What time?\"   \n",
      "5                \"Oh no. Did you try a hard reboot?\"   \n",
      "6  \"Okay, try connecting it to an external monito...   \n",
      "7  \"Let me know if that works. If not, we might h...   \n",
      "8   \"Nice! What did you think? I loved the visuals.\"   \n",
      "9  \"I can see that. The ending felt a bit rushed....   \n",
      "\n",
      "                                         target_text  \n",
      "0  \"Just saw it. They want a lot of changes to th...  \n",
      "1  \"I'll start on the revisions. Can you update t...  \n",
      "2  \"Not yet, was thinking of heading to the new b...  \n",
      "3              \"Yeah, that's the one. Want to join?\"  \n",
      "4                           \"How about around 3 PM?\"  \n",
      "5                         \"Tried it twice. Nothing.\"  \n",
      "6                  \"Good idea, let me find a cable.\"  \n",
      "7  \"Finally watched that new sci-fi movie everyon...  \n",
      "8  \"Visuals were amazing, but the plot was a bit ...  \n",
      "9  \"Definitely. Worth it just for the big screen ...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# Check essential columns\n",
    "assert 'message' in df.columns, \"Dataset must have a 'message' column.\"\n",
    "assert 'sender' in df.columns, \"Dataset must have a 'sender' column.\"\n",
    "\n",
    "# Clean sender labels (normalize)\n",
    "df['sender'] = df['sender'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Detect A/B labels dynamically\n",
    "senders = df['sender'].unique()\n",
    "print(\"Detected senders:\", senders)\n",
    "\n",
    "# Try to detect who is User A and B\n",
    "user_a = next((s for s in senders if 'a' in s), senders[0])\n",
    "user_b = next((s for s in senders if 'b' in s), senders[-1])\n",
    "\n",
    "print(f\"→ Interpreted User A as: {user_a}, User B as: {user_b}\")\n",
    "\n",
    "# Sort by timestamp if available\n",
    "if 'timestamp' in df.columns:\n",
    "    df = df.sort_values('timestamp')\n",
    "\n",
    "# Build (B → A) message pairs\n",
    "pairs = []\n",
    "for i in range(len(df) - 1):\n",
    "    sender_now = df.iloc[i]['sender']\n",
    "    sender_next = df.iloc[i + 1]['sender']\n",
    "    if re.search(user_b, sender_now) and re.search(user_a, sender_next):\n",
    "        pairs.append([df.iloc[i]['message'], df.iloc[i + 1]['message']])\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['input_text', 'target_text'])\n",
    "print(\"✅ Total pairs created:\", len(pairs_df))\n",
    "print(pairs_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c903a952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pushpit-saluja/ML/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created successfully with 10 samples.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=64):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_enc = self.tokenizer(\n",
    "            self.df.iloc[idx]['input_text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target_enc = self.tokenizer(\n",
    "            self.df.iloc[idx]['target_text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = input_enc['input_ids'].squeeze()\n",
    "        labels = target_enc['input_ids'].squeeze()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100  # ignore pad tokens\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "dataset = ChatDataset(pairs_df, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(\"DataLoader created successfully with\", len(dataset), \"samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ac1a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1: 100%|██████████| 3/3 [00:12<00:00,  4.25s/it, loss=7.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 9.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3/3 [00:11<00:00,  3.78s/it, loss=4.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 5.3603\n",
      "✅ Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Force CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training settings\n",
    "epochs = 2  # CPU is slow, keep small\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    total_loss = 0\n",
    "    for batch in loop:\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} avg loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"ChatRec_Model.pt\")\n",
    "print(\"✅ Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b2f28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Response → Hi, how are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pushpit-saluja/ML/venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 1.97476174763398e-155\n",
      "Perplexity: 46.371679435196306\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import math, numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def generate_reply(prompt, max_len=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_len,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Quick test\n",
    "print(\"Test Response →\", generate_reply(\"Hi, how are you?\"))\n",
    "\n",
    "# ---- BLEU & Perplexity ----\n",
    "def evaluate_bleu(df, n=50):\n",
    "    sample_df = df.sample(min(n, len(df)))\n",
    "    scores = []\n",
    "    for _, row in sample_df.iterrows():\n",
    "        pred = generate_reply(row[\"input_text\"])\n",
    "        ref = [row[\"target_text\"].split()]\n",
    "        scores.append(sentence_bleu(ref, pred.split(), weights=(0.5, 0.5)))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def evaluate_perplexity(loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss = model(inputs, labels=labels).loss\n",
    "            losses.append(loss.item())\n",
    "    return math.exp(np.mean(losses))\n",
    "\n",
    "print(\"BLEU:\", evaluate_bleu(pairs_df))\n",
    "print(\"Perplexity:\", evaluate_perplexity(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7be5b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Artifacts ready for submission.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model.state_dict(), \"Model.joblib\")\n",
    "\n",
    "with open(\"ReadMe.txt\", \"w\") as f:\n",
    "    f.write(\"Offline Chat-Reply Recommendation System\\n\")\n",
    "    f.write(\"Model: DistilGPT-2 fine-tuned on chat data\\n\")\n",
    "    f.write(\"Metrics: BLEU, Perplexity\\n\")\n",
    "    f.write(\"Files: ChatRec_Model.pt, Model.joblib\\n\")\n",
    "\n",
    "print(\"✅ Artifacts ready for submission.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bd61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
